{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#TODO: SAVE EACH MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Upload dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_x_speed  mean_y_speed  mean_speed     mean_acc    mean_jerk  mean_ang  \\\n",
      "0    603.874693     39.584684  607.956707  1980.903177  1373208.638 -0.807362   \n",
      "\n",
      "   mean_curve  std_x_speed  std_y_speed   std_speed  ...      max_acc  \\\n",
      "0   -0.022807   370.019319    56.034093  369.694944  ...  87490.11908   \n",
      "\n",
      "     max_ang     max_jerk  max_curve  elapsed_time  sum_of_angles  \\\n",
      "0  38.722551  5776079.105   0.160875      0.105366       0.611254   \n",
      "\n",
      "   accTimeatBeg  traj_length  numCritPoints  class  \n",
      "0      0.007992    46.307298              3      0  \n",
      "\n",
      "[1 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "#TODO: dane z gotowymi cechami -> czy próbować robić po swojemu ekstrakcję, skoro juz to zrobili?\n",
    "dataset = pd.read_csv('masterTrain.csv')\n",
    "dataset = dataset.iloc[:, 1:]\n",
    "# 377010 x 34\n",
    "print(dataset.iloc[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into users and impostors\n",
    "num_classes = len(np.unique(dataset['class']))\n",
    "\n",
    "user = {}\n",
    "impostor = {}\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    user[i] = dataset.loc[dataset['class'] == i]\n",
    "    impostor[i] = dataset.loc[dataset['class'] != i]\n",
    "    # scale impostor to the same size as user\n",
    "    impostor[i] = impostor[i].sample(n=len(user[i]))\n",
    "\n",
    "    user[i].loc[:, 'class'] = 0\n",
    "    impostor[i].loc[:, 'class'] = 1\n",
    "\n",
    "    merged_data = pd.concat([user[i], impostor[i]])\n",
    "    \n",
    "    X = merged_data.loc[:, 'mean_x_speed':'numCritPoints']\n",
    "    y = merged_data['class']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  0\n",
      "Mean Squared Error: 0.2142152772510051\n",
      "R-squared (R2): 0.14313352175954475\n",
      "Accuracy: 0.7857847227489949\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79      6575\n",
      "           1       0.81      0.75      0.78      6608\n",
      "\n",
      "    accuracy                           0.79     13183\n",
      "   macro avg       0.79      0.79      0.79     13183\n",
      "weighted avg       0.79      0.79      0.79     13183\n",
      "\n",
      "User:  1\n",
      "Mean Squared Error: 0.1992315444696397\n",
      "R-squared (R2): 0.203067700808235\n",
      "Accuracy: 0.8007684555303602\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81      7960\n",
      "           1       0.82      0.77      0.79      7916\n",
      "\n",
      "    accuracy                           0.80     15876\n",
      "   macro avg       0.80      0.80      0.80     15876\n",
      "weighted avg       0.80      0.80      0.80     15876\n",
      "\n",
      "User:  2\n",
      "Mean Squared Error: 0.21178875638841568\n",
      "R-squared (R2): 0.15280947087160912\n",
      "Accuracy: 0.7882112436115843\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.80      7290\n",
      "           1       0.82      0.74      0.78      7385\n",
      "\n",
      "    accuracy                           0.79     14675\n",
      "   macro avg       0.79      0.79      0.79     14675\n",
      "weighted avg       0.79      0.79      0.79     14675\n",
      "\n",
      "User:  3\n",
      "Mean Squared Error: 0.20654178394642336\n",
      "R-squared (R2): 0.17382975104927711\n",
      "Accuracy: 0.7934582160535766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.80     10283\n",
      "           1       0.83      0.74      0.78     10323\n",
      "\n",
      "    accuracy                           0.79     20606\n",
      "   macro avg       0.80      0.79      0.79     20606\n",
      "weighted avg       0.80      0.79      0.79     20606\n",
      "\n",
      "User:  4\n",
      "Mean Squared Error: 0.22106598984771575\n",
      "R-squared (R2): 0.11572235514286788\n",
      "Accuracy: 0.7789340101522843\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.79      7849\n",
      "           1       0.82      0.72      0.77      7911\n",
      "\n",
      "    accuracy                           0.78     15760\n",
      "   macro avg       0.78      0.78      0.78     15760\n",
      "weighted avg       0.78      0.78      0.78     15760\n",
      "\n",
      "User:  5\n",
      "Mean Squared Error: 0.23426706141349732\n",
      "R-squared (R2): 0.06277749668805721\n",
      "Accuracy: 0.7657329385865027\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76      6502\n",
      "           1       0.77      0.77      0.77      6671\n",
      "\n",
      "    accuracy                           0.77     13173\n",
      "   macro avg       0.77      0.77      0.77     13173\n",
      "weighted avg       0.77      0.77      0.77     13173\n",
      "\n",
      "User:  6\n",
      "Mean Squared Error: 0.2064047598215067\n",
      "R-squared (R2): 0.17435895786938693\n",
      "Accuracy: 0.7935952401784933\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80      5744\n",
      "           1       0.81      0.76      0.79      5685\n",
      "\n",
      "    accuracy                           0.79     11429\n",
      "   macro avg       0.79      0.79      0.79     11429\n",
      "weighted avg       0.79      0.79      0.79     11429\n",
      "\n",
      "User:  7\n",
      "Mean Squared Error: 0.21093112690828575\n",
      "R-squared (R2): 0.1562177265005461\n",
      "Accuracy: 0.7890688730917143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80      8652\n",
      "           1       0.81      0.76      0.78      8510\n",
      "\n",
      "    accuracy                           0.79     17162\n",
      "   macro avg       0.79      0.79      0.79     17162\n",
      "weighted avg       0.79      0.79      0.79     17162\n",
      "\n",
      "User:  8\n",
      "Mean Squared Error: 0.21164487930066564\n",
      "R-squared (R2): 0.15340152803891727\n",
      "Accuracy: 0.7883551206993343\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80      6264\n",
      "           1       0.81      0.74      0.78      6205\n",
      "\n",
      "    accuracy                           0.79     12469\n",
      "   macro avg       0.79      0.79      0.79     12469\n",
      "weighted avg       0.79      0.79      0.79     12469\n",
      "\n",
      "User:  9\n",
      "Mean Squared Error: 0.22062393784899248\n",
      "R-squared (R2): 0.11748238873684291\n",
      "Accuracy: 0.7793760621510075\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79      8279\n",
      "           1       0.81      0.73      0.77      8197\n",
      "\n",
      "    accuracy                           0.78     16476\n",
      "   macro avg       0.78      0.78      0.78     16476\n",
      "weighted avg       0.78      0.78      0.78     16476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models_rf = {}\n",
    "\n",
    "for i in range(num_classes):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "\n",
    "    print(\"User: \", i)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared (R2): {r2}\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    "
=======
    "Import libraries"
>>>>>>> b0b9de4f5d9e426700a8deba290ee2131ef2e168
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "best_hyperparams = []\n",
    "best_models = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "\n",
    "    hyperparams = {\n",
    "        'n_estimators': list(range(10, 200)),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4], \n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(rf, hyperparams, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print('User', i)\n",
    "    print('Best hyperparams:', grid_search.best_params_)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models.append(best_model)\n",
    "    best_hyperparams.append(grid_search.best_params_)\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared (R2): {r2}\")\n",
    "    print(classification_report(y_test, predictions))"
   ]
=======
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> b0b9de4f5d9e426700a8deba290ee2131ef2e168
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
<<<<<<< HEAD
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
=======
   "name": "python",
>>>>>>> b0b9de4f5d9e426700a8deba290ee2131ef2e168
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
