{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#TODO: SAVE EACH MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Upload dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_x_speed  mean_y_speed  mean_speed     mean_acc    mean_jerk  mean_ang  \\\n",
      "0    603.874693     39.584684  607.956707  1980.903177  1373208.638 -0.807362   \n",
      "\n",
      "   mean_curve  std_x_speed  std_y_speed   std_speed  ...      max_acc  \\\n",
      "0   -0.022807   370.019319    56.034093  369.694944  ...  87490.11908   \n",
      "\n",
      "     max_ang     max_jerk  max_curve  elapsed_time  sum_of_angles  \\\n",
      "0  38.722551  5776079.105   0.160875      0.105366       0.611254   \n",
      "\n",
      "   accTimeatBeg  traj_length  numCritPoints  class  \n",
      "0      0.007992    46.307298              3      0  \n",
      "\n",
      "[1 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "#TODO: dane z gotowymi cechami -> czy próbować robić po swojemu ekstrakcję, skoro juz to zrobili?\n",
    "dataset = pd.read_csv('masterTrain.csv')\n",
    "dataset = dataset.iloc[:, 1:]\n",
    "# 377010 x 34\n",
    "print(dataset.iloc[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into users and impostors\n",
    "num_classes = len(np.unique(dataset['class']))\n",
    "\n",
    "user = {}\n",
    "impostor = {}\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    user[i] = dataset.loc[dataset['class'] == i]\n",
    "    impostor[i] = dataset.loc[dataset['class'] != i]\n",
    "    # scale impostor to the same size as user\n",
    "    impostor[i] = impostor[i].sample(n=len(user[i]))\n",
    "\n",
    "    user[i].loc[:, 'class'] = 0\n",
    "    impostor[i].loc[:, 'class'] = 1\n",
    "\n",
    "    merged_data = pd.concat([user[i], impostor[i]])\n",
    "    \n",
    "    X = merged_data.loc[:, 'mean_x_speed':'numCritPoints']\n",
    "    y = merged_data['class']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  0\n",
      "Mean Squared Error: 0.21603580368656602\n",
      "R-squared (R2): 0.135856780281437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79      6592\n",
      "           1       0.81      0.74      0.77      6591\n",
      "\n",
      "    accuracy                           0.78     13183\n",
      "   macro avg       0.79      0.78      0.78     13183\n",
      "weighted avg       0.79      0.78      0.78     13183\n",
      "\n",
      "User:  1\n",
      "Mean Squared Error: 0.20105820105820105\n",
      "R-squared (R2): 0.1957671957671958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80      7938\n",
      "           1       0.82      0.77      0.79      7938\n",
      "\n",
      "    accuracy                           0.80     15876\n",
      "   macro avg       0.80      0.80      0.80     15876\n",
      "weighted avg       0.80      0.80      0.80     15876\n",
      "\n",
      "User:  2\n",
      "Mean Squared Error: 0.21110732538330493\n",
      "R-squared (R2): 0.15557069454568806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80      7337\n",
      "           1       0.81      0.75      0.78      7338\n",
      "\n",
      "    accuracy                           0.79     14675\n",
      "   macro avg       0.79      0.79      0.79     14675\n",
      "weighted avg       0.79      0.79      0.79     14675\n",
      "\n",
      "User:  3\n",
      "Mean Squared Error: 0.20508589731146268\n",
      "R-squared (R2): 0.1796564107541493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.80     10303\n",
      "           1       0.83      0.74      0.78     10303\n",
      "\n",
      "    accuracy                           0.79     20606\n",
      "   macro avg       0.80      0.79      0.79     20606\n",
      "weighted avg       0.80      0.79      0.79     20606\n",
      "\n",
      "User:  4\n",
      "Mean Squared Error: 0.21776649746192894\n",
      "R-squared (R2): 0.12893401015228423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.79      7880\n",
      "           1       0.82      0.72      0.77      7880\n",
      "\n",
      "    accuracy                           0.78     15760\n",
      "   macro avg       0.79      0.78      0.78     15760\n",
      "weighted avg       0.79      0.78      0.78     15760\n",
      "\n",
      "User:  5\n",
      "Mean Squared Error: 0.2292568131784711\n",
      "R-squared (R2): 0.08297274200150673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77      6586\n",
      "           1       0.77      0.78      0.77      6587\n",
      "\n",
      "    accuracy                           0.77     13173\n",
      "   macro avg       0.77      0.77      0.77     13173\n",
      "weighted avg       0.77      0.77      0.77     13173\n",
      "\n",
      "User:  6\n",
      "Mean Squared Error: 0.19756759121532944\n",
      "R-squared (R2): 0.2097296290886287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81      5715\n",
      "           1       0.83      0.77      0.80      5714\n",
      "\n",
      "    accuracy                           0.80     11429\n",
      "   macro avg       0.80      0.80      0.80     11429\n",
      "weighted avg       0.80      0.80      0.80     11429\n",
      "\n",
      "User:  7\n",
      "Mean Squared Error: 0.20819251835450414\n",
      "R-squared (R2): 0.16722992658198343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80      8581\n",
      "           1       0.81      0.76      0.78      8581\n",
      "\n",
      "    accuracy                           0.79     17162\n",
      "   macro avg       0.79      0.79      0.79     17162\n",
      "weighted avg       0.79      0.79      0.79     17162\n",
      "\n",
      "User:  8\n",
      "Mean Squared Error: 0.20827652578394418\n",
      "R-squared (R2): 0.16689389150579936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80      6234\n",
      "           1       0.82      0.75      0.78      6235\n",
      "\n",
      "    accuracy                           0.79     12469\n",
      "   macro avg       0.79      0.79      0.79     12469\n",
      "weighted avg       0.79      0.79      0.79     12469\n",
      "\n",
      "User:  9\n",
      "Mean Squared Error: 0.2176499150279194\n",
      "R-squared (R2): 0.12940033988832245\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79      8238\n",
      "           1       0.81      0.73      0.77      8238\n",
      "\n",
      "    accuracy                           0.78     16476\n",
      "   macro avg       0.79      0.78      0.78     16476\n",
      "weighted avg       0.79      0.78      0.78     16476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models_rf = {}\n",
    "\n",
    "for i in range(num_classes):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "\n",
    "    print(\"User: \", i)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared (R2): {r2}\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "User 0\n",
      "Best hyperparams: {'n_estimators': 100, 'max_features': 'log2', 'criterion': 'entropy', 'bootstrap': False}\n",
      "Best score:  0.800932977149361\n",
      "Mean Squared Error: 0.17120534021087763\n",
      "R-squared (R2): 0.3151786352160123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83      6591\n",
      "           1       0.85      0.80      0.82      6592\n",
      "\n",
      "    accuracy                           0.83     13183\n",
      "   macro avg       0.83      0.83      0.83     13183\n",
      "weighted avg       0.83      0.83      0.83     13183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "\n",
    "best_hyperparams_random = []\n",
    "best_models_random = []\n",
    "\n",
    "hyperparams = {\n",
    "        'n_estimators': [100], #np.arange(100, 500, step=50),\n",
    "        # 'max_depth': [None] + list(np.arange(10, 100, step=20)), # TODO: ograniczyć max_depth do np. 50\n",
    "        # 'min_samples_split': [2, 4, 6, 8, 10],\n",
    "        # 'min_samples_leaf': [1, 2, 4], \n",
    "        'criterion':['gini','entropy'],\n",
    "        'max_features': ['log2', 'sqrt'],\n",
    "        'bootstrap': [False]\n",
    "    }\n",
    "\n",
    "classes = list(range(num_classes))\n",
    "random.shuffle(classes)\n",
    "random_classes = classes[:3]\n",
    "\n",
    "for i in random_classes:\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    random_search = RandomizedSearchCV(rf, hyperparams, cv=5, scoring='accuracy', n_jobs=-1, n_iter=10)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    print('User ', i)\n",
    "    print('Best hyperparams:', random_search.best_params_)\n",
    "    print('Best score: ', random_search.best_score_)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_models_random.append(best_model)\n",
    "    best_hyperparams_random.append(random_search.best_params_)\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared (R2): {r2}\")\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_grid = []\n",
    "best_models_grid = []\n",
    "\n",
    "hyperparams = {\n",
    "        'n_estimators': list(range(10, 200)),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4], \n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "for i in range(num_classes):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(rf, hyperparams, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print('User ', i)\n",
    "    print('Best hyperparams:', grid_search.best_params_)\n",
    "    print('Best score: ', random_search.best_score_)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models_grid.append(best_model)\n",
    "    best_hyperparams_grid.append(grid_search.best_params_)\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared (R2): {r2}\")\n",
    "    print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
